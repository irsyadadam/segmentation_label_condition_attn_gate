{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3619ca1-2294-4bba-8bbc-6935d551610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6155a350-fb2c-49f5-aa07-04cfd8764943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testImages  trainImages  trainMasks  trainSet.csv\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b32552-3736-48be-934c-8a589a910f07",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922f5d59-ac32-4d11-b29b-fe569ec3f5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageID</th>\n",
       "      <th>status</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1164</td>\n",
       "      <td>1</td>\n",
       "      <td>16165 16166 16167 16168 16169 16678 16679 1668...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1171</td>\n",
       "      <td>1</td>\n",
       "      <td>58682 58683 58684 58685 58686 59194 59195 5919...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1177</td>\n",
       "      <td>1</td>\n",
       "      <td>125642 125643 125644 125645 125646 126155 1261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1178</td>\n",
       "      <td>1</td>\n",
       "      <td>53951 53952 53953 53954 53955 54463 54464 5446...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>20408</td>\n",
       "      <td>1</td>\n",
       "      <td>61293 61294 61295 61296 61297 61804 61805 6180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>20410</td>\n",
       "      <td>1</td>\n",
       "      <td>78295 78805 78806 78807 79316 79317 79318 7931...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20594</td>\n",
       "      <td>1</td>\n",
       "      <td>61197 61198 61199 61200 61201 61709 61710 6171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>20605</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>20692</td>\n",
       "      <td>1</td>\n",
       "      <td>110405 110406 110407 110408 110409 110917 1109...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     imageID  status                                               mask\n",
       "0       1164       1  16165 16166 16167 16168 16169 16678 16679 1668...\n",
       "1       1169       0                                               -100\n",
       "2       1171       1  58682 58683 58684 58685 58686 59194 59195 5919...\n",
       "3       1177       1  125642 125643 125644 125645 125646 126155 1261...\n",
       "4       1178       1  53951 53952 53953 53954 53955 54463 54464 5446...\n",
       "..       ...     ...                                                ...\n",
       "500    20408       1  61293 61294 61295 61296 61297 61804 61805 6180...\n",
       "501    20410       1  78295 78805 78806 78807 79316 79317 79318 7931...\n",
       "502    20594       1  61197 61198 61199 61200 61201 61709 61710 6171...\n",
       "503    20605       0                                               -100\n",
       "504    20692       1  110405 110406 110407 110408 110409 110917 1109...\n",
       "\n",
       "[505 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "train_df = pd.read_csv(data_dir + \"trainSet.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbf335a-6264-491a-8f21-de05d131b919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "1    352\n",
       "0    153\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e697271-cd0a-4d19-875f-94ff4c433808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'data/trainImages/trainImages': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls data/trainImages/trainImages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5344cb9-f160-4017-ba83-3106353e1ca4",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076c7859-f955-455a-87de-3357f96d3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: width = 512, height = 512\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = \"data/trainImages/4826.jpg\"\n",
    "\n",
    "# Open image\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Get original size (width, height)\n",
    "width, height = img.size\n",
    "\n",
    "print(f\"Original size: width = {width}, height = {height}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61157329-b1ad-4047-985d-c9b10a597a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class LungCTNeedleDatasetV2(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, image_size=(512, 512), use_ignore_index=True):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = image_size  # (H, W)\n",
    "        self.use_ignore_index = use_ignore_index  # True → fill mask with -100 when label == 0\n",
    "\n",
    "\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),  # [1, H, W]\n",
    "        ])\n",
    "\n",
    "\n",
    "    def _parse_mask(self, mask_str, label):\n",
    "        H, W = self.image_size\n",
    "        if str(mask_str).strip() == \"-100\" or label == 0:\n",
    "            fill_value = -100.0 if self.use_ignore_index else 0.0\n",
    "            return torch.full((1, H, W), fill_value, dtype=torch.float32)\n",
    "\n",
    "        mask = torch.zeros(H * W, dtype=torch.float32)\n",
    "        try:\n",
    "            indices = list(map(int, mask_str.strip().split()))\n",
    "            indices = [i for i in indices if 0 <= i < H * W]\n",
    "            mask[indices] = 1.0\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Failed parsing mask: {mask_str} — {e}\")\n",
    "        return mask.view(1, H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        patient_id = str(row['imageID'])\n",
    "        label = int(row['status'])\n",
    "        mask_str = row['mask']\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_dir, f\"{patient_id}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        image = self.image_transform(image)  # [1, H, W]\n",
    "\n",
    "        # Create mask\n",
    "        mask = self._parse_mask(mask_str, label)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.float32), mask, patient_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9cb8776-9936-49b0-8857-cbd38b2277dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LungCTNeedleDatasetV2(\n",
    "    csv_path=\"data/trainSet.csv\",\n",
    "    image_dir=\"data/trainImages\",\n",
    "    image_size=(512, 512),\n",
    "    use_ignore_index=True  # set to False if you want zero-filled instead\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85302787-f6a6-46a5-b15c-02febd4f03c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 512, 512])\n",
      "torch.Size([4, 1, 512, 512])\n",
      "tensor([1., 1., 1., 0.])\n",
      "('2603', '2321', '2826', '19285')\n"
     ]
    }
   ],
   "source": [
    "for image, label, mask, patient_id in dataloader:\n",
    "    print(image.shape)     # [B, 1, 512, 512]\n",
    "    print(mask.shape)      # [B, 1, 512, 512]\n",
    "    print(label) \n",
    "    print(patient_id)      # List[str]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56547507-c3c4-4fa5-aaa6-da67dead4429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageID</th>\n",
       "      <th>status</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>16042</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     imageID  status  mask\n",
       "378    16042       0  -100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"imageID\"] == 16042]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5238da-aae1-4ceb-afd7-4da4871899b1",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e729603d-4ce8-415a-b9c0-491b7f2a1553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/iadam/.cache/torch/hub/Warvito_radimagenet-models_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pretrained_backbone = torch.hub.load(\"Warvito/radimagenet-models\", 'radimagenet_resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b5c18b-4952-4ec5-961d-1aeae5241ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c8b484-137a-45b5-96f8-7666c19db1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from radimagenet_models.models.resnet import radimagenet_resnet50\n",
    "\n",
    "\n",
    "class AttentionGatedUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, feature_dim=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Encoder (RadImageNet pretrained) ---\n",
    "        self.encoder = radimagenet_resnet50()\n",
    "\n",
    "        # Patch first conv to accept grayscale\n",
    "        if in_channels == 1:\n",
    "            old_conv = self.encoder.conv1\n",
    "            new_conv = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            with torch.no_grad():\n",
    "                new_conv.weight = nn.Parameter(old_conv.weight.mean(dim=1, keepdim=True))\n",
    "            self.encoder.conv1 = new_conv\n",
    "\n",
    "        # Encoder blocks\n",
    "        self.enc1 = nn.Sequential(self.encoder.conv1, self.encoder.bn1, self.encoder.relu)  # [B, 64, 256, 256]\n",
    "        self.enc2 = self.encoder.layer1  # [B, 256, 128, 128]\n",
    "        self.enc3 = self.encoder.layer2  # [B, 512, 64, 64]\n",
    "        self.enc4 = self.encoder.layer3  # [B, 1024, 32, 32]\n",
    "        self.enc5 = self.encoder.layer4  # [B, 2048, 16, 16]\n",
    "\n",
    "        # --- Attention Gate ---\n",
    "        self.attn_gate = nn.Sequential(\n",
    "            nn.Conv2d(feature_dim, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # --- Decoder Blocks (U-Net style) ---\n",
    "        self.up4 = self._upblock(2048, 1024)  # 16 → 32\n",
    "        self.up3 = self._upblock(1024, 512)   # 32 → 64\n",
    "        self.up2 = self._upblock(512, 256)    # 64 → 128\n",
    "        self.up1 = self._upblock(256, 64)     # 128 → 256\n",
    "        self.up0 = self._upblock(64, 32)      # 256 → 512 (new learnable final upsample)\n",
    "\n",
    "        # --- Heads ---\n",
    "        self.segmentation_head = nn.Conv2d(32, 1, kernel_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "\n",
    "    def _upblock(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def _align_skip(self, x, skip):\n",
    "        if x.shape[-2:] != skip.shape[-2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        return x + skip\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Encoder ---\n",
    "        e1 = self.enc1(x)       # [B, 64, 256, 256]\n",
    "        e2 = self.enc2(e1)      # [B, 256, 128, 128]\n",
    "        e3 = self.enc3(e2)      # [B, 512, 64, 64]\n",
    "        e4 = self.enc4(e3)      # [B, 1024, 32, 32]\n",
    "        e5 = self.enc5(e4)      # [B, 2048, 16, 16]\n",
    "\n",
    "        # --- Attention Gating ---\n",
    "        attn = self.attn_gate(e5)     # [B, 1, 16, 16]\n",
    "        gated = e5 * attn             # [B, 2048, 16, 16]\n",
    "\n",
    "        # --- Decoder ---\n",
    "        d4 = self._align_skip(self.up4(gated), e4)  # → [B, 1024, 32, 32]\n",
    "        d3 = self._align_skip(self.up3(d4), e3)     # → [B, 512, 64, 64]\n",
    "        d2 = self._align_skip(self.up2(d3), e2)     # → [B, 256, 128, 128]\n",
    "        d1 = self._align_skip(self.up1(d2), e1)     # → [B, 64, 256, 256]\n",
    "        d0 = self.up0(d1)                           # → [B, 32, 512, 512]\n",
    "\n",
    "        # --- Outputs ---\n",
    "        seg_mask = self.segmentation_head(d0)       # → [B, 1, 512, 512]\n",
    "        class_logits = self.classifier(gated).squeeze(-1)  # → [B]\n",
    "\n",
    "        return {\n",
    "            \"segmentation\": seg_mask,\n",
    "            \"attention\": attn,\n",
    "            \"classification\": class_logits\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "398e154e-91cf-449f-9e25-550ba0503933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_bce_loss(pred, target, ignore_val=-100.0):\n",
    "    \"\"\"\n",
    "    pred: [B, 1, H, W] — logits\n",
    "    target: [B, 1, H, W] — binary mask with some pixels = ignore_val\n",
    "    \"\"\"\n",
    "    mask = (target != ignore_val).float()\n",
    "    target_clean = torch.clamp(target, min=0.0, max=1.0)\n",
    "\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target_clean, reduction='none')\n",
    "    bce = bce * mask\n",
    "\n",
    "    return bce.sum() / (mask.sum() + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a7eaf50-41b2-4d7c-8c95-9e2526313dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_one_epoch_with_eval(model, dataloader, optimizer, device, scaler, lambda_attn=1.0, lambda_cls=1.0, threshold=0.5):\n",
    "    model.train()\n",
    "    cls_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    dice_total = 0.0\n",
    "    sens_total = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, labels, masks, _ = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            seg_pred = outputs['segmentation']\n",
    "            attn_map = outputs['attention']\n",
    "            class_logits = outputs['classification']\n",
    "\n",
    "            if class_logits.ndim == 2:\n",
    "                class_logits = class_logits.squeeze(-1)\n",
    "\n",
    "            # --- Losses ---\n",
    "            loss_seg = masked_bce_loss(seg_pred, masks)\n",
    "            loss_cls = cls_loss_fn(class_logits, labels)\n",
    "            attn_neg = attn_map[labels == 0]\n",
    "            loss_attn = attn_neg.mean() if attn_neg.numel() > 0 else torch.tensor(0.0, device=device)\n",
    "            loss = loss_seg + lambda_cls * loss_cls + lambda_attn * loss_attn\n",
    "\n",
    "        # --- Backward + Optimizer ---\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # --- Metrics (only compute for positives) ---\n",
    "        needle_present = labels == 1\n",
    "        if needle_present.sum() > 0:\n",
    "            seg_pred_bin = torch.sigmoid(seg_pred[needle_present]) > threshold\n",
    "            gt_mask = masks[needle_present]\n",
    "\n",
    "            preds_flat = seg_pred_bin.view(seg_pred_bin.size(0), -1)\n",
    "            masks_flat = gt_mask.view(gt_mask.size(0), -1)\n",
    "\n",
    "            intersection = (preds_flat * masks_flat).sum(dim=1)\n",
    "            dice = (2. * intersection) / (preds_flat.sum(dim=1) + masks_flat.sum(dim=1) + 1e-8)\n",
    "            TP = intersection\n",
    "            FN = ((~preds_flat) * masks_flat.bool()).sum(dim=1)\n",
    "            sens = TP / (TP + FN + 1e-8)\n",
    "\n",
    "            dice_total += dice.sum().item()\n",
    "            sens_total += sens.sum().item()\n",
    "            count += preds_flat.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_dice = dice_total / count if count > 0 else 0.0\n",
    "    avg_sens = sens_total / count if count > 0 else 0.0\n",
    "\n",
    "    return avg_loss, avg_dice, avg_sens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c1e465-c268-4973-b956-04fd678368ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc22d22289b444d19300fc3eb285cf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iadam/miniconda3/envs/thyroid_vision_CUDA/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.3450 | Dice: 0.0104 | Sensitivity: 0.1584\n",
      "Epoch 2 | Loss: 1.1028 | Dice: 0.0099 | Sensitivity: 0.0091\n",
      "Epoch 3 | Loss: 1.0294 | Dice: 0.0024 | Sensitivity: 0.0017\n",
      "Epoch 4 | Loss: 0.9256 | Dice: 0.0028 | Sensitivity: 0.0023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m scaler \u001b[38;5;241m=\u001b[39m GradScaler()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[0;32m---> 15\u001b[0m     loss, dice, sens \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_with_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Dice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Sensitivity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msens\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m, in \u001b[0;36mtrain_one_epoch_with_eval\u001b[0;34m(model, dataloader, optimizer, device, scaler, lambda_attn, lambda_cls, threshold)\u001b[0m\n\u001b[1;32m     30\u001b[0m loss_seg \u001b[38;5;241m=\u001b[39m masked_bce_loss(seg_pred, masks)\n\u001b[1;32m     31\u001b[0m loss_cls \u001b[38;5;241m=\u001b[39m cls_loss_fn(class_logits, labels)\n\u001b[0;32m---> 32\u001b[0m attn_neg \u001b[38;5;241m=\u001b[39m \u001b[43mattn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m loss_attn \u001b[38;5;241m=\u001b[39m attn_neg\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mif\u001b[39;00m attn_neg\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_seg \u001b[38;5;241m+\u001b[39m lambda_cls \u001b[38;5;241m*\u001b[39m loss_cls \u001b[38;5;241m+\u001b[39m lambda_attn \u001b[38;5;241m*\u001b[39m loss_attn\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = \"cuda:0\"\n",
    "num_epochs = 200\n",
    "train_loader = dataloader\n",
    "\n",
    "#v1: epochs = 100, no scheduler\n",
    "\n",
    "model = AttentionGatedUNet(in_channels=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    loss, dice, sens = train_one_epoch_with_eval(model, train_loader, optimizer, device, scaler)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Dice: {dice:.4f} | Sensitivity: {sens:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88757933-5446-4a26-83ed-c2db4882f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    loss, dice, sens = train_one_epoch_with_eval(model, train_loader, optimizer, device, scaler)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Dice: {dice:.4f} | Sensitivity: {sens:.4f}\")\n",
    "    scheduler.step(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416e310-a4ae-46e8-9a51-fcf3d9224311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38066022-2cca-4078-99e0-2737fd96437b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc9714-a7e5-4442-90f2-9a0b5442812d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b37dfe-40f7-4423-af11-7d047e8c1cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thyroid_CUDA",
   "language": "python",
   "name": "thyroid_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
